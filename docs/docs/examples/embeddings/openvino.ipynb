{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Embeddings with OpenVINO\n",
    "\n",
    "[OpenVINOâ„¢](https://github.com/openvinotoolkit/openvino) is an open-source toolkit for optimizing and deploying AI inference. The OpenVINOâ„¢ Runtime supports various hardware [devices](https://github.com/openvinotoolkit/openvino?tab=readme-ov-file#supported-hardware-matrix) including x86 and ARM CPUs, and Intel GPUs. It can help to boost deep learning performance in Computer Vision, Automatic Speech Recognition, Natural Language Processing and other common tasks.\n",
    "\n",
    "Hugging Face embedding model can be supported by OpenVINO through ``OpenVINOEmbedding`` class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exporter\n",
    "\n",
    "It is possible to export your model to the OpenVINO IR format with `create_and_save_openvino_model` function, and load the model from local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import OpenVINOEmbedding\n",
    "\n",
    "OpenVINOEmbedding.create_and_save_openvino_model(\n",
    "    \"BAAI/bge-small-en-v1.5\", \"./bge_ov\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "If you have an Intel GPU, you can specify `device=\"gpu\"` to run inference on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_embed_model = OpenVINOEmbedding(model_id_or_path=\"./bge_ov\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ov_embed_model.get_text_embedding(\"Hello World!\")\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiModal Model Exporter\n",
    "\n",
    "Class `OpenVINOMultiModalEmbedding` can support exporting and loading [`open_clip`](https://github.com/mlfoundations/open_clip) models with OpenVINO runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import (\n",
    "    OpenVINOMultiModalEmbedding,\n",
    ")\n",
    "\n",
    "OpenVINOMultiModalEmbedding.create_and_save_openvino_model(\n",
    "    model_id=\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\",\n",
    "    output_path=\"ViT-B-32-ov\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiModal Model Loading\n",
    "\n",
    "If you have an Intel GPU, you can specify `device=\"GPU\"` to run inference on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_embed_model = OpenVINOMultiModalEmbedding(\n",
    "    model_path=\"./ViT-B-32-ov\", device=\"CPU\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed images and queries with OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "image_url = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcStMP8S3VbNCqOQd7QQQcbvC_FLa1HlftCiJw&s\"\n",
    "im = Image.open(requests.get(image_url, stream=True).raw)\n",
    "print(\"Image:\")\n",
    "display(im)\n",
    "\n",
    "im.save(\"logo.jpg\")\n",
    "image_embeddings = ov_embed_model.get_image_embedding(\"logo.jpg\")\n",
    "print(\"Image dim:\", len(image_embeddings))\n",
    "print(\"Image embed:\", image_embeddings[:5])\n",
    "\n",
    "text_embeddings = ov_embed_model.get_text_embedding(\n",
    "    \"Logo of a pink blue llama on dark background\"\n",
    ")\n",
    "print(\"Text dim:\", len(text_embeddings))\n",
    "print(\"Text embed:\", text_embeddings[:5])\n",
    "\n",
    "cos_sim = dot(image_embeddings, text_embeddings) / (\n",
    "    norm(image_embeddings) * norm(text_embeddings)\n",
    ")\n",
    "print(\"Cosine similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information refer to:\n",
    "\n",
    "* [OpenVINO LLM guide](https://docs.openvino.ai/2024/learn-openvino/llm_inference_guide.html).\n",
    "\n",
    "* [OpenVINO Documentation](https://docs.openvino.ai/2024/home.html).\n",
    "\n",
    "* [OpenVINO Get Started Guide](https://www.intel.com/content/www/us/en/content-details/819067/openvino-get-started-guide.html).\n",
    "\n",
    "* [RAG example with LlamaIndex](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/llm-rag-llamaindex)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
